(130, 5)
(130, 512, 512, 3)
presprocessing done
tf graph made
session starts
step: 1 > training- loss: 1.03999996185
step: 2 > training- loss: 1.08000004292
step: 3 > training- loss: 1.08000004292
step: 4 > training- loss: 1.0
step: 5 > training- loss: 1.03999996185
step: 6 > training- loss: 0.959999978542
step: 7 > training- loss: 1.03999996185
step: 8 > training- loss: 1.0
step: 9 > training- loss: 1.05999994278
step: 10 > training- loss: 1.08000004292
step: 11 > training- loss: 1.03999996185
step: 12 > training- loss: 1.01999998093
step: 13 > training- loss: 1.05999994278
step: 14 > training- loss: 1.12000000477
step: 15 > training- loss: 1.05999994278
step: 16 > training- loss: 1.01999998093
step: 17 > training- loss: 1.0
step: 18 > training- loss: 1.01999998093
step: 19 > training- loss: 1.05999994278
step: 20 > training- loss: 1.03999996185
step: 21 > training- loss: 1.03999996185
step: 22 > training- loss: 1.03999996185
step: 23 > training- loss: 1.03999996185
step: 24 > training- loss: 1.10000002384
step: 25 > training- loss: 1.03999996185
step: 26 > training- loss: 0.939999997616
step: 27 > training- loss: 0.959999978542
step: 28 > training- loss: 1.05999994278
step: 29 > training- loss: 1.03999996185
step: 30 > training- loss: 1.05999994278
step: 31 > training- loss: 1.05999994278
step: 32 > training- loss: 0.980000019073
step: 33 > training- loss: 0.980000019073
step: 34 > training- loss: 1.03999996185
step: 35 > training- loss: 1.01999998093
step: 36 > training- loss: 0.939999997616
step: 37 > training- loss: 1.05999994278
step: 38 > training- loss: 1.13999998569
step: 39 > training- loss: 1.08000004292
step: 40 > training- loss: 1.08000004292
step: 41 > training- loss: 1.05999994278
step: 42 > training- loss: 1.01999998093
step: 43 > training- loss: 1.01999998093
step: 44 > training- loss: 1.08000004292
step: 45 > training- loss: 1.05999994278
step: 46 > training- loss: 1.15999996662
step: 47 > training- loss: 1.01999998093
step: 48 > training- loss: 1.08000004292
step: 49 > training- loss: 1.0
step: 50 > training- loss: 1.0
step: 51 > training- loss: 1.08000004292
step: 52 > training- loss: 0.959999978542
step: 53 > training- loss: 1.08000004292
step: 54 > training- loss: 1.03999996185
step: 55 > training- loss: 1.03999996185
step: 56 > training- loss: 1.12000000477
step: 57 > training- loss: 1.01999998093
step: 58 > training- loss: 1.01999998093
step: 59 > training- loss: 1.0
step: 60 > training- loss: 1.01999998093
step: 61 > training- loss: 1.01999998093
step: 62 > training- loss: 1.05999994278
step: 63 > training- loss: 0.980000019073
step: 64 > training- loss: 1.0
step: 65 > training- loss: 1.01999998093
step: 66 > training- loss: 1.0
step: 67 > training- loss: 1.10000002384
step: 68 > training- loss: 1.01999998093
step: 69 > training- loss: 1.05999994278
step: 70 > training- loss: 1.01999998093
step: 71 > training- loss: 1.03999996185
step: 72 > training- loss: 0.959999978542
step: 73 > training- loss: 1.12000000477
step: 74 > training- loss: 1.0
step: 75 > training- loss: 0.939999997616
step: 76 > training- loss: 1.01999998093
step: 77 > training- loss: 0.959999978542
step: 78 > training- loss: 0.959999978542
step: 79 > training- loss: 1.05999994278
step: 80 > training- loss: 0.920000016689
step: 81 > training- loss: 0.980000019073
step: 82 > training- loss: 1.01999998093
step: 83 > training- loss: 1.01999998093
step: 84 > training- loss: 0.980000019073
step: 85 > training- loss: 1.05999994278
step: 86 > training- loss: 1.01999998093
step: 87 > training- loss: 1.05999994278
step: 88 > training- loss: 0.959999978542
step: 89 > training- loss: 1.01999998093
step: 90 > training- loss: 1.0
step: 91 > training- loss: 1.01999998093
step: 92 > training- loss: 0.980000019073
step: 93 > training- loss: 1.0
step: 94 > training- loss: 1.08000004292
step: 95 > training- loss: 1.03999996185
step: 96 > training- loss: 1.0
step: 97 > training- loss: 0.980000019073
step: 98 > training- loss: 1.03999996185
step: 99 > training- loss: 1.05999994278
step: 100 > training- loss: 1.01999998093
step: 101 > training- loss: 1.10000002384
step: 102 > training- loss: 1.01999998093
step: 103 > training- loss: 1.05999994278
step: 104 > training- loss: 1.0
step: 105 > training- loss: 0.939999997616
step: 106 > training- loss: 1.05999994278
step: 107 > training- loss: 1.12000000477
step: 108 > training- loss: 1.0
step: 109 > training- loss: 1.08000004292
step: 110 > training- loss: 1.03999996185
step: 111 > training- loss: 1.12000000477
step: 112 > training- loss: 1.0
step: 113 > training- loss: 1.01999998093
step: 114 > training- loss: 1.05999994278
step: 115 > training- loss: 1.10000002384
step: 116 > training- loss: 1.01999998093
step: 117 > training- loss: 0.959999978542
step: 118 > training- loss: 1.05999994278
step: 119 > training- loss: 1.0
step: 120 > training- loss: 1.01999998093
step: 121 > training- loss: 1.08000004292
step: 122 > training- loss: 0.899999976158
step: 123 > training- loss: 1.01999998093
step: 124 > training- loss: 1.10000002384
step: 125 > training- loss: 1.0
step: 126 > training- loss: 1.10000002384
step: 127 > training- loss: 1.08000004292
step: 128 > training- loss: 1.03999996185
step: 129 > training- loss: 1.03999996185
step: 130 > training- loss: 1.05999994278
step: 131 > training- loss: 1.03999996185
step: 132 > training- loss: 1.05999994278
step: 133 > training- loss: 1.0
step: 134 > training- loss: 1.01999998093
step: 135 > training- loss: 1.15999996662
step: 136 > training- loss: 1.12000000477
step: 137 > training- loss: 1.0
step: 138 > training- loss: 1.01999998093
step: 139 > training- loss: 1.0
step: 140 > training- loss: 1.0
step: 141 > training- loss: 1.05999994278
step: 142 > training- loss: 1.08000004292
step: 143 > training- loss: 0.980000019073
step: 144 > training- loss: 0.920000016689
step: 145 > training- loss: 1.0
step: 146 > training- loss: 1.0
step: 147 > training- loss: 0.920000016689
step: 148 > training- loss: 1.03999996185
step: 149 > training- loss: 1.08000004292
step: 150 > training- loss: 1.05999994278
step: 151 > training- loss: 0.959999978542
step: 152 > training- loss: 1.0
step: 153 > training- loss: 1.01999998093
step: 154 > training- loss: 0.939999997616
step: 155 > training- loss: 1.03999996185
step: 156 > training- loss: 1.01999998093
step: 157 > training- loss: 1.03999996185
step: 158 > training- loss: 1.08000004292
step: 159 > training- loss: 1.08000004292
step: 160 > training- loss: 0.980000019073
step: 161 > training- loss: 1.01999998093
step: 162 > training- loss: 1.10000002384
step: 163 > training- loss: 1.03999996185
step: 164 > training- loss: 1.0
step: 165 > training- loss: 1.10000002384
step: 166 > training- loss: 1.01999998093
step: 167 > training- loss: 1.03999996185
step: 168 > training- loss: 1.03999996185
step: 169 > training- loss: 1.03999996185
step: 170 > training- loss: 1.08000004292
step: 171 > training- loss: 1.0
step: 172 > training- loss: 1.08000004292
step: 173 > training- loss: 1.01999998093
step: 174 > training- loss: 1.01999998093
step: 175 > training- loss: 1.08000004292
step: 176 > training- loss: 0.980000019073
step: 177 > training- loss: 0.939999997616
step: 178 > training- loss: 1.0
step: 179 > training- loss: 1.0
step: 180 > training- loss: 1.05999994278
step: 181 > training- loss: 1.03999996185
step: 182 > training- loss: 1.0
step: 183 > training- loss: 0.959999978542
step: 184 > training- loss: 1.03999996185
step: 185 > training- loss: 0.959999978542
step: 186 > training- loss: 1.0
step: 187 > training- loss: 1.0
step: 188 > training- loss: 1.08000004292
step: 189 > training- loss: 1.0
step: 190 > training- loss: 1.01999998093
step: 191 > training- loss: 1.12000000477
step: 192 > training- loss: 1.13999998569
step: 193 > training- loss: 1.01999998093
step: 194 > training- loss: 1.10000002384
step: 195 > training- loss: 1.03999996185
step: 196 > training- loss: 1.0
step: 197 > training- loss: 0.959999978542
step: 198 > training- loss: 0.959999978542
step: 199 > training- loss: 1.0
step: 200 > training- loss: 1.01999998093
step: 201 > training- loss: 1.08000004292
step: 202 > training- loss: 1.01999998093
step: 202 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 203 > training- loss: 1.0
step: 203 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 204 > training- loss: 1.03999996185
step: 204 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 205 > training- loss: 1.08000004292
step: 205 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 206 > training- loss: 0.980000019073
step: 206 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 207 > training- loss: 1.0
step: 207 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 208 > training- loss: 1.10000002384
step: 208 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 209 > training- loss: 1.01999998093
step: 209 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 210 > training- loss: 1.03999996185
step: 210 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 211 > training- loss: 1.0
step: 211 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 212 > training- loss: 1.01999998093
step: 212 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 213 > training- loss: 1.01999998093
step: 213 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 214 > training- loss: 0.980000019073
step: 214 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 215 > training- loss: 1.10000002384
step: 215 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 216 > training- loss: 1.0
step: 216 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 217 > training- loss: 1.05999994278
step: 217 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 218 > training- loss: 0.939999997616
step: 218 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 219 > training- loss: 1.03999996185
step: 219 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 220 > training- loss: 1.10000002384
step: 220 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 221 > training- loss: 1.05999994278
step: 221 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 222 > training- loss: 1.01999998093
step: 222 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 223 > training- loss: 1.0
step: 223 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 224 > training- loss: 1.05999994278
step: 224 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 225 > training- loss: 1.03999996185
step: 225 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 226 > training- loss: 1.03999996185
step: 226 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 227 > training- loss: 1.13999998569
step: 227 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 228 > training- loss: 1.05999994278
step: 228 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 229 > training- loss: 1.01999998093
step: 229 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 230 > training- loss: 1.05999994278
step: 230 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 231 > training- loss: 1.08000004292
step: 231 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 232 > training- loss: 1.01999998093
step: 232 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 233 > training- loss: 1.08000004292
step: 233 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 234 > training- loss: 1.01999998093
step: 234 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 235 > training- loss: 1.05999994278
step: 235 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 236 > training- loss: 1.12000000477
step: 236 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 237 > training- loss: 1.05999994278
step: 237 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 238 > training- loss: 1.01999998093
step: 238 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 239 > training- loss: 1.01999998093
step: 239 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 240 > training- loss: 1.08000004292
step: 240 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 241 > training- loss: 0.980000019073
step: 241 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 242 > training- loss: 1.01999998093
step: 242 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 243 > training- loss: 1.03999996185
step: 243 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 244 > training- loss: 1.05999994278
step: 244 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 245 > training- loss: 1.12000000477
step: 245 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 246 > training- loss: 1.05999994278
step: 246 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 247 > training- loss: 1.05999994278
step: 247 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 248 > training- loss: 0.980000019073
step: 248 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 249 > training- loss: 1.08000004292
step: 249 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 250 > training- loss: 0.980000019073
step: 250 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 251 > training- loss: 1.03999996185
step: 251 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 252 > training- loss: 1.0
step: 252 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 253 > training- loss: 1.01999998093
step: 253 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 254 > training- loss: 1.0
step: 254 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 255 > training- loss: 1.03999996185
step: 255 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 256 > training- loss: 1.01999998093
step: 256 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 257 > training- loss: 1.01999998093
step: 257 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 258 > training- loss: 1.01999998093
step: 258 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 259 > training- loss: 1.01999998093
step: 259 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 260 > training- loss: 1.05999994278
step: 260 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 261 > training- loss: 1.01999998093
step: 261 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 262 > training- loss: 1.03999996185
step: 262 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 263 > training- loss: 1.05999994278
step: 263 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 264 > training- loss: 1.05999994278
step: 264 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 265 > training- loss: 1.10000002384
step: 265 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 266 > training- loss: 1.01999998093
step: 266 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 267 > training- loss: 1.08000004292
step: 267 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 268 > training- loss: 1.03999996185
step: 268 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 269 > training- loss: 1.0
step: 269 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 270 > training- loss: 1.01999998093
step: 270 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 271 > training- loss: 1.03999996185
step: 271 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 272 > training- loss: 1.03999996185
step: 272 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 273 > training- loss: 0.980000019073
step: 273 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 274 > training- loss: 1.08000004292
step: 274 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 275 > training- loss: 1.01999998093
step: 275 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 276 > training- loss: 1.08000004292
step: 276 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 277 > training- loss: 1.01999998093
step: 277 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 278 > training- loss: 1.01999998093
step: 278 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 279 > training- loss: 1.10000002384
step: 279 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 280 > training- loss: 0.980000019073
step: 280 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 281 > training- loss: 1.05999994278
step: 281 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 282 > training- loss: 0.980000019073
step: 282 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 283 > training- loss: 1.0
step: 283 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 284 > training- loss: 1.08000004292
step: 284 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 285 > training- loss: 1.08000004292
step: 285 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 286 > training- loss: 1.08000004292
step: 286 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 287 > training- loss: 1.01999998093
step: 287 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 288 > training- loss: 1.03999996185
step: 288 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 289 > training- loss: 1.03999996185
step: 289 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 290 > training- loss: 1.03999996185
step: 290 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 291 > training- loss: 1.05999994278
step: 291 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 292 > training- loss: 1.12000000477
step: 292 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 293 > training- loss: 1.01999998093
step: 293 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 294 > training- loss: 1.03999996185
step: 294 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 295 > training- loss: 1.01999998093
step: 295 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 296 > training- loss: 0.959999978542
step: 296 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 297 > training- loss: 0.980000019073
step: 297 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 298 > training- loss: 1.03999996185
step: 298 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 299 > training- loss: 1.01999998093
step: 299 > training- f1_score: 0.816326530612,accuracy: 0.7
step: 300 > training- loss: 1.0
step: 300 > training- f1_score: 0.816326530612,accuracy: 0.7
